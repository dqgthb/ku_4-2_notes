# 수업

Error를 허용하는 classifier를 수학적으로 찾게 되는데, 어느정도로 허용하게 되는가?
Margin을 넓힐 수도 있고 좁힐 수도 있다.

약간의 non-linear함을 찾는 방법은? Soft margin classifier라고 한다. 거기에 대응해서,
이것을 우리는 hard margin이라고 한다.

- max        margin classifier (hard)
- soft margin classifier (약간의 misclassification 허용)

이 안에 있는 것들을 support vector라고 합니다. 데이터를 가지고 어느것이 서포트 벡터인지를
찾아낼 수가 있고, sum 하고 dot product를 해서 찾아냅니다. 이게 soft margin classifier
에요. 그런데 3번째는 3개를 나눠봅시다.

## soft margin classifier (support vector classifier)

## support vector machine

input space를 커널을 통해서 변형을 시킵니다.

## Feature selection

모든 combination을 테스트 해보는 거죠. 더 빨리 하기 위해서...
라고 하는 것들이 기본적인 방법들이에요~

구체적인 알고리즘 설명 x

일반적으로 봤을때, NaiveBayes, IBk, J48

Feature selection 이랑 디시전 트리는 왜 성능 차이가 별로 안날까?

디시전 트리는 내부에서 알아서  feature selection을 하는 느낌이라서?

** 시험문제. 나이브 베이지언은 피처 10개를 다 쓴다~ Likelihood를 다 붙였잖아요.
근데 그 중에는 irrelevant 한 것도 있고, redundant한 것도 있어요.

그런데 디시전 트리는 그 중에 각각 하나하나랑 피처들을 평가를 해서 엔트로피 값이 가장
좋은 것을 골라서 나누고 나눈다. 그래서 얘가 스스로 좋은 것들을 차례대로 골라서 한다.
그게 Feature selection이랑 거의 비슷합니다.

그래서 우선 디시전 트리를 돌려버리고 그 중에서 몇개 좋은 feature를 select하는 방법도
있다. 어떤 알고리즘을 돌려서 결과가 어떻게 나오는지를 여러분들이 분석하고 생각할 수 있으면
굉장히 좋은 insight를 얻을 수 있습니다.

## Dimension Reduction

feature selection과 feature extraction 방법 중에서 feature extraction의 한 종류이다.

### Principle Component Analysis

Linear regression을 해서, authogonal line으로 본다. 그러면, 차원을 이제 2차원에서 1차원으로
줄일 수가 있습니다. 복잡한 문제는 아니고, 훨씬 명확하게 데이터들을 구분을 할 수 있어요.

x, y 축으로 볼 때는 잘 모르는데,

주성분 분석을 해버리면 한 차원은 엄청 잘 벌어지고 차원 1개는 사라져버린다.

PCA 기법들입니다.

Feature가 너무 많아! PCA가 너무 많으면 줄여버린다. 그래서 시각적으로 보기 위해서 사용하기도
합니다. 그 예가 어디에 있냐면, 여기 있네요. MNIST데이터를 가지고 이미지를 구분하는 건데요,
오리지널 데이터는 784의 오리지널 데이터를 가지고 있어요. 154개를 가지고 한다고 했을때, 
PCA는 이렇게 차이가 났어요. 84개를 두개의 차원으로 확 줄어들었어요.

각각의 색깔들이 보입니다.

문제들도 있어요. 표현하기 힘든 상황들도 있어요. 그럼에도 불구하고 유용합니다.

## Ensemble

Decision tree를 만들었어요. 저번에 본 것처럼 각이 있어서 좀 민감하죠. 그런데, 이것을 하나만 쓰는게 아니라,
여러개를 쓰면, 

### Bagging

여러개를 넣어서 위원들을 만듭니다. Data set을 가지고 모델1도 만들고 모델n도 만듭니다.
얘는 yes라고 하고 얘는 no라고 한다. 이게 Bagging 방법인데요. 어떻게 다른 모델이 나오게
할 수 있을까? Data sampling을 다르게 하거나, 알고리즘 파라미터를 다르게 만든다.
다수결?

Single Decision Tree vs. 500 small Decision Trees bagging

It looks like a nonlinear classifier.

### 시험문제!: 난리니어 클래시피케이션을 만드는 다양한 방법을 얘기해보시오.

- Linear한 것을 합쳐서 만들수도 있고
- Input space를 변형해서 만들 수도 있고
- 3번째는 앙상블도 그렇게 볼 수도 있겠네요

### Boosting

하나의 모델을 만들어요. misclassify한 모델도 있잖아요. 거기에 가중치를 둬가지고 잘 못하는 것을
잘하도록 만드는 가능성이 있는거죠. 그래서 위원들은 약간의 다른 장점과 단점을 서로 상쇄할 수 있는
형태를 가지게 되는 거에요. 얘네들이 모여서 다수결 의사결정을 합니다. 위에꺼는 서로 상쇄할 수 있는
지는 잘 모르겠으나, 그렇게 될거라고 믿고 랜덤 샘플링 하는거에요. 다양성을 확보하는 거죠.

## Random Forest

가장 인기있는 모델 중에 하나입니다. 해석하기가 힘들어요. 디시젼 트리는 잘 설명을 해주는데,
랜덤 포레스트는 잘 못한다.

### 데이터를 랜더마이징하는게 아니라, 알고리즘을 랜더마이징한다.

### Random forests

### Weka: trees > randomForests


다음 시간에 3가지 방법, 부스팅 방법, 요새 굉장히 뜨고 있는 방법 많이 있거든요? 그리고 스태킹
알고리즘들을 살펴보도록 합시다.
